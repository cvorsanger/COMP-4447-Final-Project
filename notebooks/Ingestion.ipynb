{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Section 2: Data Ingestion\n",
    "\n",
    "- [Section 2.1: Motivation](#motivation)\n",
    "- [Section 2.2: Ingestion](#ingestion)\n",
    "- [Section 2.3: Sample ans Explanation](#sample)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 2.1: Motivation <a class=\"anchor\" id=\"motivation\"></a>\n",
    "\n",
    "<p>\n",
    "<img src = https://kss.rs/eng_new/wp-content/uploads/2020/05/tokio.jpg style=\"width:175px;height:250px;margin:0px 10px;\" ALIGN=\"right\" />\n",
    "    <p>A particular interest of the authors is the ongoing Summer Olympic games. We enjoy watching the top athletes in the world compete in sports that are not on television very often; sports like, gymnastics,swimming and track. A natural curioisty than was to see hiw other people view the Olympics.</p>    \n",
    "    <p>As social media has grown Twitter has been the go to platform for the world to express their views. Twitter allows for people to express their feelings on just about any subject they desire (for better or for worst). It would make sence then to look at Twitter data to model the sentiment around the Olympics. Luckily Twitter provides an API  for us to query historical tweet data.</p>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 2.2: Ingestion <a class=\"anchor\" id=\"ingestion\"></a>\n",
    "\n",
    "The twitter API is a relative easy to use API. There are a few restictions such as rate limits and API types that you do have to consider. For the purposes of this study, the API makes it easy to look at historical tweets containing specific words and hashtags. The follow details certain aspects of the Twitter API that pertains to this project. For more information please visit [here](https://developer.twitter.com/en/docs/twitter-api)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step is to get authorization. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests \n",
    "# Save Authorization Info.\n",
    "client_key = 'k7IeaeoVVSVVRs2VMRyjWMGhB'\n",
    "client_secret = 'QjWhmf3ylIKXTYn6zv26tTkPjobjbapTKlC4JDay74jd647mcl'\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAO3%2BQgEAAAAAKO%2BcZLOP%2B1jh1SeWDdIMpCF4smc%3DiUFDhj7SZfb1fl177n5Fipd9OA2Elj9aVdezk2hhBUGbTrdLgY'\n",
    "key_secret = '{}:{}'.format(client_key, client_secret).encode('ascii')\n",
    "b64_encoded_key = base64.b64encode(key_secret)\n",
    "b64_encoded_key = b64_encoded_key.decode('ascii')\n",
    "\n",
    "# Build API URL \n",
    "base_url = 'https://api.twitter.com/'\n",
    "auth_endpoint = base_url + 'oauth2/token'\n",
    "auth_headers = { 'Authorization': 'Basic {}'.format(b64_encoded_key),\n",
    "                'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8' }\n",
    "auth_data = { 'grant_type': 'client_credentials' }\n",
    "\n",
    "# Provide Authorization Info and Save Access Token\n",
    "response = requests.post(auth_endpoint, headers=auth_headers, data=auth_data)\n",
    "print(\"Response Status Code: \",response.status_code)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected we get a response status code of \"200\". This tells us that we succesfully recieved our access token.\n",
    "\n",
    "Lets get the access token from our response and save it in a variable *access_token*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "json_data =  response.json()\n",
    "access_token = json_data['access_token']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A utility function was then created. This function allows us to use the twitter API to save tweets. The function needs the users saved access token and allows for the user to specify the number of tweet to pull. The most important parameter of this function is the query parameter. This allows for you filter tweets you recieve by specifing hashtags, words, retweets, etc. For this project we are concerned with tweets containg the hashtags \"#olympics\" and containing the a specific sport."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_tweets(access_token, query, max_tweets=10, tweet_limit=10):\n",
    "    \"\"\"Retrieve tweets from the recent search API.\n",
    "        Args\n",
    "        ----\n",
    "        access_token (str): A valid bearer token for making Twitter API requests.\n",
    "        query (str): A valid Twitter query string for filtering the search tweets.\n",
    "        max_tweets (int): The maximum number of tweets to collect in total.\n",
    "        tweet_limit (int): The number of maximum tweets per API request. \n",
    "    \"\"\"\n",
    "\n",
    "    page_token = None\n",
    "    tweet_data = []\n",
    "\n",
    "    search_headers = {\n",
    "        'Authorization': 'Bearer {}'.format(access_token),\n",
    "        'User-Agent': 'v2FullArchiveSearchPython',\n",
    "    }\n",
    "\n",
    "    # Divides the max tweets into the appropriate number of requests based on the tweet_limit.\n",
    "    for i in range(max_tweets // tweet_limit - 1):\n",
    "        search_parameters = {\n",
    "            'query': query,\n",
    "            'max_results': tweet_limit,\n",
    "            'tweet.fields': 'lang,created_at,referenced_tweets,source,conversation_id'\n",
    "        }\n",
    "\n",
    "        print(f'Request {i + 1}: {query}')\n",
    "        \n",
    "        # If we reach the 2nd page of results, add a next_token attribute to the search parameters\n",
    "        if i > 0:\n",
    "            search_parameters['next_token'] = page_token\n",
    "\n",
    "        response = requests.get(search_url, headers=search_headers, params=search_parameters)\n",
    "        if response.status_code != 200:\n",
    "            print(f'\\tError occurred: Status Code{response.status_code}: {response.text}')\n",
    "        else:\n",
    "            # We need to check for a result count before doing anything futher; if we have result_count we have data\n",
    "            if response.json()['meta']['result_count'] > 0:\n",
    "                tweet_data.extend(response.json()['data'])\n",
    "                \n",
    "                # If a 'next_token' exists, then update the page token to continue pagination through results\n",
    "                if 'next_token' in response.json()['meta']:\n",
    "                    page_token = response.json()['meta']['next_token']\n",
    "            else:\n",
    "                print(f'\\tNo data returned for query!')\n",
    "                break\n",
    "        \n",
    "        print(f'\\t{len(tweet_data)} total tweets gathered')\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    return tweet_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the above function we can sample retweets having to do with olympic gymnastics. Notice that we are specify the hashtag \"#olympics\", the word \"gymnastics\" and is a retweet in the query parameter. Finally, so we do have to keep requesting data from the twitter API we save the tweets in a pickle file. This way we can use them later in our anaysis (and twitter will not get made at us)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make the request\n",
    "q = '#olympics gymnastics -is:retweet'\n",
    "olympic_tweets = get_tweets(access_token, q, max_tweets, tweet_limit)\n",
    "\n",
    "with open('../data/olympic-gynmastics.pkl', 'wb') as f:\n",
    "\tpickle.dump(olympic_tweets, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are also concerned with the onversation tweets (replies) that follow the original tweet. Tweet also provides a conversation id attribute with their tweets. This allows us to reuse the twitter API to get all the tweets in a conversation. Using the same function we can get all of the replies by specifing this conversation ID on the query parameter. Once again using olympic gymnastics as an example..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gynmastics_tweets = pd.read_pickle(\"../data/olympic-gynmastics.pkl\")\n",
    "\n",
    "conversation_ids = [ tweet['conversation_id'] for tweet in gynmastics_tweets ]\n",
    "len(conversation_ids)\n",
    "\n",
    "# Query all replies\n",
    "replies = []\n",
    "\n",
    "for id in conversation_ids[0:11]:\n",
    "\tq = 'conversation_id:{id}'.format(id=id)\n",
    "\treplies.extend(get_tweets(access_token, q, max_tweets, tweet_limit))\n",
    "\n",
    "with open('../data/olympic-gynmastics-replies.pkl', 'wb') as f:\n",
    "\tpickle.dump(replies, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 2.3: Sample and Explanation <a class=\"anchor\" id=\"sample\"></a>\n",
    "\n",
    "Alright we have pulled in some tweets. Now lets have a look at some of the original tweets and the retweets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.set_option('expand_frame_repr', False)\n",
    "\n",
    "original_tweets_df = pd.DataFrame(olympic_tweets)\n",
    "original_tweets_df.sample(5)\n",
    "\n",
    "response_tweet_df = pd.DataFrame(replies)\n",
    "response_tweet_df.sample(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are a lot of information about tweets that you recieve. For us the most important ones are:\n",
    "\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"container\">\n",
    "   <div style=\"float:left;width:20%\"><a href=\"./Intro.ipynb\"><< Section 1: Introduction</a></div>\n",
    "   <div style=\"float:right;width:20%\"><a href=\"./Cleaning.ipynb\">Section 3: Data Cleaning >></a></div>\n",
    "   <div style=\"float:right;width:40%\"><a href=\"../main.md\">Table of Contents</a></div>\n",
    "</div>"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}