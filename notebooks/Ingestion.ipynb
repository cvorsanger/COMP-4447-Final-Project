{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Section 2: Data Ingestion\n",
    "\n",
    "- [Section 2.1: Motivation](#motivation)\n",
    "- [Section 2.2: Ingestion](#ingestion)\n",
    "- [Section 2.3: Sample ans Explanation](#sample)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 2.1: Motivation <a class=\"anchor\" id=\"motivation\"></a>\n",
    "\n",
    "<p>\n",
    "<img src = https://kss.rs/eng_new/wp-content/uploads/2020/05/tokio.jpg style=\"width:175px;height:250px;margin:0px 10px;\" ALIGN=\"right\" />\n",
    "    <p>A particular interest of the authors is the ongoing Summer Olympic games. We enjoy watching the top athletes in the world compete in sports that are not on television very often; sports like, gymnastics, swimming, and track. A natural curiosity than was to see how other people view the Olympics.</p>    \n",
    "    <p>As social media has grown Twitter has been the go-to platform for the world to express their views. Twitter allows people to express their feelings on just about any subject they desire (for better or for worst). It would make sense then to look at Twitter data to model the sentiment around the Olympics. Luckily, Twitter provides an API  for us to query historical tweet data.</p>\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 2.2: Ingestion <a class=\"anchor\" id=\"ingestion\"></a>\n",
    "\n",
    "The Twitter API is a relatively easy-to-use API. There are a few restrictions such as rate limits and API types that you do have to consider. For the purposes of this study, the API makes it easy to look at historical tweets containing specific words and hashtags. The following details certain aspects of the Twitter API that pertains to this project. For more information please visit [here](https://developer.twitter.com/en/docs/twitter-api)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step is to get authorization. To get this we must provide the API with our given client key and secret using the POST command with the requests library. Once we get POST we will receive a response from the API we must save."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests \n",
    "import base64\n",
    "import time\n",
    "\n",
    "# Save Authorization Info.\n",
    "client_key = 'XXXXXXXXXXXXXXXXXXXXXXXXXX' \n",
    "client_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXX' \n",
    "bearer_token = 'XXXXXXXXXXXXXXXXXXXXXXXXXX' \n",
    "key_secret = '{}:{}'.format(client_key, client_secret).encode('ascii')\n",
    "b64_encoded_key = base64.b64encode(key_secret)\n",
    "b64_encoded_key = b64_encoded_key.decode('ascii')\n",
    "# Build API URL \n",
    "base_url = 'https://api.twitter.com/'\n",
    "auth_endpoint = base_url + 'oauth2/token'\n",
    "auth_headers = { 'Authorization': 'Basic {}'.format(b64_encoded_key),\n",
    "                'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8' }\n",
    "auth_data = { 'grant_type': 'client_credentials' }\n",
    "# Provide Authorization Info and Save Access Token\n",
    "response = requests.post(auth_endpoint, headers=auth_headers, data=auth_data)\n",
    "print(\"Response Status Code: \",response.status_code)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Response Status Code:  200\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected, we get a response status code of \"200\". This tells us that we succesfully recieved our response. The next step is to get the *access_token* from our authorization response."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "json_data =  response.json()\n",
    "access_token = json_data['access_token']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A utility function was then created. This function allows us to use the Twitter API to save tweets. The function needs the user's saved access token and allows for the user to specify the number of tweets to pull. The most important parameter of this function is the query parameter. This allows you to filter the tweets you receive by specific hashtags, words, retweets, etc. For this project, we are concerned with tweets containing the hashtags \"#olympics\" and containing the specific sport."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_tweets(access_token, query, max_tweets=10, tweet_limit=10):\n",
    "    \"\"\"Retrieve tweets from the recent search API.\n",
    "        Args\n",
    "        ----\n",
    "        access_token (str): A valid bearer token for making Twitter API requests.\n",
    "        query (str): A valid Twitter query string for filtering the search tweets.\n",
    "        max_tweets (int): The maximum number of tweets to collect in total.\n",
    "        tweet_limit (int): The number of maximum tweets per API request. \n",
    "    \"\"\"\n",
    "    page_token = None\n",
    "    tweet_data = []\n",
    "    search_headers = {\n",
    "        'Authorization': 'Bearer {}'.format(access_token),\n",
    "        'User-Agent': 'v2FullArchiveSearchPython',\n",
    "    }\n",
    "    # Divides the max tweets into the appropriate number of requests based on the tweet_limit.\n",
    "    for i in range(max_tweets // tweet_limit - 1):\n",
    "        search_parameters = {\n",
    "            'query': query,\n",
    "            'max_results': tweet_limit,\n",
    "            'tweet.fields': 'lang,created_at,referenced_tweets,source,conversation_id'\n",
    "        }\n",
    "                # If we reach the 2nd page of results, add a next_token attribute to the search parameters\n",
    "        if i > 0:\n",
    "            search_parameters['next_token'] = page_token\n",
    "\n",
    "        response = requests.get(search_url, headers=search_headers, params=search_parameters)\n",
    "        if response.status_code != 200:\n",
    "            print(f'\\tError occurred: Status Code{response.status_code}: {response.text}')\n",
    "        else:\n",
    "            # Check for a result count before doing anything futher; if we have result_count we have data\n",
    "            if response.json()['meta']['result_count'] > 0:\n",
    "                tweet_data.extend(response.json()['data'])\n",
    "                \n",
    "                # If a 'next_token' exists, then update the page token to continue pagination\n",
    "                if 'next_token' in response.json()['meta']:\n",
    "                    page_token = response.json()['meta']['next_token']\n",
    "            else:\n",
    "                print(f'\\tNo data returned for query!')\n",
    "                break        \n",
    "        print(f'\\t{len(tweet_data)} total tweets gathered')\n",
    "        time.sleep(1)\n",
    "    return tweet_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the above function we can sample retweets having to do with Olympic gymnastics. Notice that we specify the hashtag \"#olympics\", the word \"gymnastics\" and is a retweet in the query parameter. Finally, so we do have to keep requesting data from the Twitter API we save the tweets in a pickle file. This way we can use them later in our analysis (and Twitter will not get mad at us for rate-limits)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make the request\n",
    "q = '#olympics gymnastics -is:retweet'\n",
    "olympic_tweets = get_tweets(access_token, q, max_tweets=5000, tweet_limit=100)\n",
    "#Save the data\n",
    "with open('../data/gymnastics-tweets.pkl', 'wb') as f:\n",
    "\tpickle.dump(olympic_tweets, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the \"get_tweets\" function we tried to pull 5000 tweets for the sports basketball, biking, diving, gymnastics, skateboarding, surfing, track, and volleyball. This was easily done by changing the query parameter to include the name of the sport."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 2.3: Sample and Explanation <a class=\"anchor\" id=\"sample\"></a>\n",
    "\n",
    "Alright we have pulled in some tweets. Now lets have a look at a sample of the tweets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "original_tweets_df = pd.DataFrame(pd.read_pickle('../data/gymnastics-tweets.pkl'))\n",
    "original_tweets_df.sample(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text                   id  \\\n",
       "1754  @Shawn_Shipp_ Thank You, Simone Biles - Headph...  1423371158443941890   \n",
       "1349  Unexpected fun fact:  \\nLiu Yang originally wa...  1423471713312854017   \n",
       "389         Rhythmic Gymnastics looks insane. #Olympics  1424213988720717828   \n",
       "1055  @kpkuehler Thank You, Simone Biles - Headphone...  1423796825761456128   \n",
       "1004  @Simone_Biles you are a wonderful human being ...  1423812938889142273   \n",
       "\n",
       "                    created_at      conversation_id  \n",
       "1754  2021-08-05T19:51:43.000Z  1423299151215931392  \n",
       "1349  2021-08-06T02:31:17.000Z  1423471713312854017  \n",
       "389   2021-08-08T03:40:50.000Z  1424213988720717828  \n",
       "1055  2021-08-07T00:03:10.000Z  1423616706413572103  \n",
       "1004  2021-08-07T01:07:12.000Z  1423812938889142273  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>conversation_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>@Shawn_Shipp_ Thank You, Simone Biles - Headph...</td>\n",
       "      <td>1423371158443941890</td>\n",
       "      <td>2021-08-05T19:51:43.000Z</td>\n",
       "      <td>1423299151215931392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>Unexpected fun fact:  \\nLiu Yang originally wa...</td>\n",
       "      <td>1423471713312854017</td>\n",
       "      <td>2021-08-06T02:31:17.000Z</td>\n",
       "      <td>1423471713312854017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>Rhythmic Gymnastics looks insane. #Olympics</td>\n",
       "      <td>1424213988720717828</td>\n",
       "      <td>2021-08-08T03:40:50.000Z</td>\n",
       "      <td>1424213988720717828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>@kpkuehler Thank You, Simone Biles - Headphone...</td>\n",
       "      <td>1423796825761456128</td>\n",
       "      <td>2021-08-07T00:03:10.000Z</td>\n",
       "      <td>1423616706413572103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>@Simone_Biles you are a wonderful human being ...</td>\n",
       "      <td>1423812938889142273</td>\n",
       "      <td>2021-08-07T01:07:12.000Z</td>\n",
       "      <td>1423812938889142273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is a lot of information about tweets that one can recieve from the API. We have narrowed it down to the important ones for this study:\n",
    "\n",
    "- text - The text of the tweet sent out. This will include user handles, hashtags, emojis, and any retweet indicators\n",
    "- id - The unique id given to the tweet. This allows for twitter to store tweets.\n",
    "- created_at - When the tweet was tweeted. Given in UTC time.\n",
    "- conversation_id - The unique id given to Twitter conversations. We can use this id to get all tweets in a conversation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"container\">\n",
    "   <div style=\"float:left;width:20%\"><a href=\"./Intro.ipynb\"><< Section 1: Introduction</a></div>\n",
    "   <div style=\"float:right;width:20%\"><a href=\"./Cleaning.ipynb\">Section 3: Data Cleaning >></a></div>\n",
    "   <div style=\"float:right;width:40%\"><a href=\"../main.md\">Table of Contents</a></div>\n",
    "</div>"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5740b0c65ce1535d49f8104bf53e2dbf28aba8cf44c23304f5a7991b903c1ba"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}