{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Section 3: Data Cleaning\n",
    "- [Section 3.1: Initial Exploration](#explor)\n",
    "- [Section 3.2: Type Conversion](#type)\n",
    "- [Section 3.3: Missing Values](#missing)\n",
    "- [Section 3.4: Outliers](#outs)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 3.1: Initial Exploration <a class=\"anchor\" id=\"explor\"></a>\n",
    "\n",
    "So, now we have tweets regarding the olympic sports of basketball, biking, diving, gymnastics, skateboarding, surfing, track, and volleyball. All og the tweets were collected into a single dataframe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "olympic_df = pd.DataFrame(columns=['id', 'created_at', 'conversation_id', 'text'])\n",
    "for s in queries.keys():\n",
    "\tprint(f'Deserializing {s}')\n",
    "\tdf = pd.DataFrame(pd.read_pickle('./data/' + s + '-tweets.pkl'))\n",
    "\tdf['sport'] = s\n",
    "\tolympic_df = olympic_df.append(df, ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have all of the tweets in single dataframe, making it easier for us to look at all of the tweets and clean them in bucnhes. During this process a new variable was created, sport. This varibale will track for us the sport that each tweet is talking about. When we make our sentiment analysis model later this will be very handy when anayzing by sport."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "olympic_df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the shape function we find that we have 30544 tweets. Time to get our hands dirty and clean the data.\n",
    "\n",
    "As with all NLP data there is alot of cleaning that can be done. For this project we will be concentrating on removal of certain emtities such as url links and hastags, removal of stopwords, and lemmatization. After this NLP specific cleaning we should be ready to analyze the sentiment of tweets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 3.2: Type Conversion <a class=\"anchor\" id=\"type\"></a>\n",
    "\n",
    "We needed to change a few data types. Thankfully, the tweet text was already given to us as an object (string). To make our analysis easier we changed the tweet id and the conversation ids of type int from tyoe object. To facilitate the time aspect of our anaysis we needed to also covert the created_at variable to a datetime object."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "olympic_df.id = olympic_df.id.astype('int32')\n",
    "olympic_df.created_at = pd.to_datetime(olympic_df.created_at)\n",
    "olympic_df.conversation_id = olympic_df.conversation_id.astype('int32')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This leaves us with these variable types the correct variable types."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "olympic_df.dtypes()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 3.3: Missing Values <a class=\"anchor\" id=\"missing\"></a>\n",
    "\n",
    "Twitter keeps track of everything about a tweet. This is good news to us because the twitter API rarely gives you missing values. The below plot shows that there is no missing values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import missingno as msno\n",
    "msno(olympic_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because we do not have any missing values we do not have to fill in any of these values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 3.4: Outliers <a class=\"anchor\" id=\"outs\"></a>\n",
    "\n",
    "With text data it can be difficult to define what an outlier is. Obviously be can say obnoxiously positive and negative stuff without really nowing anything; this *definatly* doesn't happen on twitter (wink wink). However, the only real way to detict this is after the sentiment model is built. Also how do we know what are genuine tweets taht should be factored in our anaysis and bogus tweets that should not. We do assume tha most people that are tweeting about the Olympics are at least watching them. This is also our main target audience in this study; average peeople watching the Olympics. With it being impossible to detect bogus tweets from the API and our assumption that a vast majoroity of our tweets originate from our target audience, we decide to anaylze all of the tweets we could get. Potentially this could skew our sentiment scores. However, we feel confident that with the number of good tweets this potential skew effect should be drowned out."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"container\">\n",
    "   <div style=\"float:left;width:20%\"><a href=\"./Ingestion.ipynb\"><< Section 2: Data Ingestion</a></div>\n",
    "   <div style=\"float:right;width:20%\"><a href=\"./Model.ipynb\">Section 4: Model Creation >></a></div>\n",
    "   <div style=\"float:right;width:40%\"><a href=\"../main.md\">Table of Contents</a></div>\n",
    "</div>"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}