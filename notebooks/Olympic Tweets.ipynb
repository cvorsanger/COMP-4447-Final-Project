{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{center}\n",
    "    BY\\\\\n",
    "    Carl Ausmees\\\\\n",
    "    Troy Jennings\\\\\n",
    "    Corey Vorsanger\\\\\n",
    "    \\medskip\n",
    "    COMP 4447: Data Science Tools 1\\\\\n",
    "    \\bigskip\n",
    "    \\bigskip\n",
    "    \\textit{For a more interactive report experience please go to \\href{https://github.com/cvorsanger/COMP-4447-Final-Project}{https://github.com/cvorsanger/COMP-4447-Final-Project} and launch the binder link}\n",
    "\\end{center}\n",
    "\n",
    "\\newpage\n",
    "\n",
    "\\tableofcontents"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\section{Introduction}\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Research Question}\n",
    "\n",
    "Every four years (five years sometimes) something special happens. No, it is not the world's greatest athletes coming together to showcase elite human athletisism in\n",
    "the Summer Olympics. Rather, it is the millions of people that come together to tweet about these athletes. In a showcase of exceptional human thumbs, these people tell you all you need to know about the Olympics; with 100% accuracy of course. \n",
    "\n",
    "So what does the twitter-sphere have to say about the Olympics? Are there sports that are being talked about in a better light then others? How does the sentiment of the Olympics change over the course of the event? In this project we will be investigating tweet sentiment involving Olympic sports to see how they evolve over time. In the end the sentiment of specfic sports will be analyzed and any trends should be discovered."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Literature Review}\n",
    "\n",
    "Text sentiment analysis is really nothing new. With the advancements of Natuaral Languauge Processing (NLP) techniques building a sentiment analysis tool as become common place in a wide variety of applications. While sentiment analysis is not limited to Twitter and can be used for any text data, with its vast amount of text data and constant addition of data Twitter has been used in many sentiment analysis applications. From using Twitter data to help predict stock data as in [Anshul Mittal](http://cs229.stanford.edu/proj2011/GoelMittal-StockMarketPredictionUsingTwitterSentimentAnalysis.pdf) to the always civil discussions about politics as shown in the [Geeks for Geeks article](https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/) and [TSAM](https://ieeexplore.ieee.org/abstract/document/6581022?casa_token=qrfkpDZP30sAAAAA:PNpsFf2_T9jXUB81SKLMldZji2tDprsCz4Ec4QSrHxlJQNIW3Yi52tWHZ4jhfTgPrqRjzjdKBfA) . Unsurprisingly, there has been work done in the Olympic domain. The closest work we have discovered documented in a paper was in [2016 Olympic Games on Twitter: Sentiment Analysis of Sports Fans Tweets using Big Data Framework](../Literature%20Review/JACET_Volume%205_Issue%203_Page%20143-160.pdf). This paper came out of the Ilamic Azad University in Iran. The writter focus on Iranian Olympians instead of sports. Using tweets in both English and Farsi they classify tweets as fearful, angry, surprising, sadness, joyful, neatruel, and anticipation. We will be only concerend about classifying tweets as negative, positive, or neutral.\n",
    "\n",
    "While most work follows the same general outline there are differences in sentiement analysis works. For instance [2016 Olympic Games on Twitter: Sentiment Analysis of Sports Fans Tweets using Big Data Framework](../Literature%20Review/JACET_Volume%205_Issue%203_Page%20143-160.pdf) uses the WordNet package to handle most of their preprocessing and sentiment analysis model. [Geeks for Geeks article](https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/) uses the TextBlob library for their sentiment analysis. [Yalin Yener](https://towardsdatascience.com/step-by-step-twitter-sentiment-analysis-in-python-d6f650ade58d) introduces the Vader sentiment engine in the nltk library.\n",
    "\n",
    "After review, we have decide to prusue using mostly the Natural Language ToolKit(NLTK) and the Textblob libraries. With these libraries we should be able to do most of the data cleaning and allows us to use the Vader sentiment engine. More information on Vader will be presented in the Model Creation section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\section{Data Ingestion}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Motivation}\n",
    "\n",
    "A particular interest of the authors are the ongoing Summer Olympic games. We enjoy watching the top athletes in the world compete in sports that are not on television very often; sports like, gymnastics,swimming, and track. A natural curioisty than was to see how other people view the Olympics.As social media has grown Twitter has been the go to platform for the world to express their views. Twitter allows for people to express their feelings on just about any subject they desire (for better or for worst). It would make sence then to look at Twitter data to model the sentiment around the Olympics. Luckily, Twitter provides an API  for us to query historical tweet data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Ingestion}\n",
    "\n",
    "he twitter API is a relative easy to use API. There are a few restictions such as rate limits and API types that you do have to consider. For the purposes of this study, the API makes it easy to look at historical tweets containing specific words and hashtags. The follow details certain aspects of the Twitter API that pertains to this project. For more information please visit \\href{https://developer.twitter.com/en/docs/twitter-api}{here}\n",
    "\n",
    "The first step is to get authorization. To get this we must provide the API with our given client key and secret using the POST command with the requests library. Once we get POST we will recieve a response from the API we must save."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests \n",
    "import base64\n",
    "# Save Authorization Info.\n",
    "client_key = 'XXXXXXXXXXXXXXXXXXXXXXXXXX' \n",
    "client_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXX' \n",
    "bearer_token = 'XXXXXXXXXXXXXXXXXXXXXXXXXX' \n",
    "key_secret = '{}:{}'.format(client_key, client_secret).encode('ascii')\n",
    "b64_encoded_key = base64.b64encode(key_secret)\n",
    "b64_encoded_key = b64_encoded_key.decode('ascii')\n",
    "# Build API URL \n",
    "base_url = 'https://api.twitter.com/'\n",
    "auth_endpoint = base_url + 'oauth2/token'\n",
    "auth_headers = { 'Authorization': 'Basic {}'.format(b64_encoded_key),\n",
    "                'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8' }\n",
    "auth_data = { 'grant_type': 'client_credentials' }\n",
    "# Provide Authorization Info and Save Access Token\n",
    "response = requests.post(auth_endpoint, headers=auth_headers, data=auth_data)\n",
    "print(\"Response Status Code: \",response.status_code)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected we get a response status code of \"200\". This tells us that we succesfully recieved our responce.\n",
    "\n",
    "The next step is to get the *access_token* from our authorization response."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A utility function was then created. This function allows us to use the twitter API to save tweets. The function needs the users saved access token and allows for the user to specify the number of tweest to pull. The most important parameter of this function is the query parameter. This allows for you filter the tweets you recieve by specific hashtags, words, retweets, etc. For this project we are concerned with tweets containg the hashtags \"#olympics\" and containing the a specific sport."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_tweets(access_token, query, max_tweets=10, tweet_limit=10):\n",
    "    \"\"\"Retrieve tweets from the recent search API.\n",
    "        Args\n",
    "        ----\n",
    "        access_token (str): A valid bearer token for making Twitter API requests.\n",
    "        query (str): A valid Twitter query string for filtering the search tweets.\n",
    "        max_tweets (int): The maximum number of tweets to collect in total.\n",
    "        tweet_limit (int): The number of maximum tweets per API request. \n",
    "    \"\"\"\n",
    "    page_token = None\n",
    "    tweet_data = []\n",
    "    search_headers = {\n",
    "        'Authorization': 'Bearer {}'.format(access_token),\n",
    "        'User-Agent': 'v2FullArchiveSearchPython',\n",
    "    }\n",
    "    # Divides the max tweets into the appropriate number of requests based on the tweet_limit.\n",
    "    for i in range(max_tweets // tweet_limit - 1):\n",
    "        search_parameters = {\n",
    "            'query': query,\n",
    "            'max_results': tweet_limit,\n",
    "            'tweet.fields': 'lang,created_at,referenced_tweets,source,conversation_id'\n",
    "        }\n",
    "                # If we reach the 2nd page of results, add a next_token attribute to the search parameters\n",
    "        if i > 0:\n",
    "            search_parameters['next_token'] = page_token\n",
    "\n",
    "        response = requests.get(search_url, headers=search_headers, params=search_parameters)\n",
    "        if response.status_code != 200:\n",
    "            print(f'\\tError occurred: Status Code{response.status_code}: {response.text}')\n",
    "        else:\n",
    "            # We need to check for a result count before doing anything futher; if we have result_count we have data\n",
    "            if response.json()['meta']['result_count'] > 0:\n",
    "                tweet_data.extend(response.json()['data'])\n",
    "                \n",
    "                # If a 'next_token' exists, then update the page token to continue pagination through results\n",
    "                if 'next_token' in response.json()['meta']:\n",
    "                    page_token = response.json()['meta']['next_token']\n",
    "            else:\n",
    "                print(f'\\tNo data returned for query!')\n",
    "                break        \n",
    "        print(f'\\t{len(tweet_data)} total tweets gathered')\n",
    "        time.sleep(1)\n",
    "    return tweet_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the above function we can sample retweets having to do with olympic gymnastics. Notice that we are specify the hashtag \"#olympics\", the word \"gymnastics\" and is a retweet in the query parameter. Finally, so we do have to keep requesting data from the twitter API we save the tweets in a pickle file. This way we can use them later in our anaysis (and twitter will not get mad at us for rate limits)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make the request\n",
    "q = '#olympics gymnastics -is:retweet'\n",
    "olympic_tweets = get_tweets(access_token, q, max_tweets=5000, tweet_limit=100)\n",
    "\n",
    "with open('../data/gymnastics-tweets.pkl', 'wb') as f:\n",
    "\tpickle.dump(olympic_tweets, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the \"get_tweets\" function we tried to pull 5000 tweets for the sports basketball, biking, diving, gymnastics, skateboarding, surfing, track, and volleyball. This was easily done by changing the query parameter to include the name of the sport."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Sample and Explanation}\n",
    "\n",
    "Alright we have pulled in some tweets. Now lets have a look at some of the original tweets and the retweets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "original_tweets_df = pd.DataFrame(pd.read_pickle('../data/gymnastics-tweets.pkl'))\n",
    "original_tweets_df.sample(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text                   id  \\\n",
       "1883  @EntradaBooks Thank You, Simone Biles - Headph...  1423371668039389192   \n",
       "1000  @RealSamimAktar @stephanietara Thank You, Simo...  1423814245183987712   \n",
       "1646  #Wrestlympics - Gymnastics: Floor Exercise\\n\\n...  1423387934217146374   \n",
       "283   U can drop anything to win gold. But pls ensur...  1424333312454135812   \n",
       "5     Is it a bird?! Is it a plane?!\\nNo it's some o...  1425760249936588802   \n",
       "\n",
       "                    created_at      conversation_id  \n",
       "1883  2021-08-05T19:53:45.000Z  1423253491586318338  \n",
       "1000  2021-08-07T01:12:23.000Z  1423659289458089986  \n",
       "1646  2021-08-05T20:58:23.000Z  1423386446879203328  \n",
       "283   2021-08-08T11:34:59.000Z  1424333312454135812  \n",
       "5     2021-08-12T10:05:07.000Z  1425760249936588802  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>conversation_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>@EntradaBooks Thank You, Simone Biles - Headph...</td>\n",
       "      <td>1423371668039389192</td>\n",
       "      <td>2021-08-05T19:53:45.000Z</td>\n",
       "      <td>1423253491586318338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>@RealSamimAktar @stephanietara Thank You, Simo...</td>\n",
       "      <td>1423814245183987712</td>\n",
       "      <td>2021-08-07T01:12:23.000Z</td>\n",
       "      <td>1423659289458089986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>#Wrestlympics - Gymnastics: Floor Exercise\\n\\n...</td>\n",
       "      <td>1423387934217146374</td>\n",
       "      <td>2021-08-05T20:58:23.000Z</td>\n",
       "      <td>1423386446879203328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>U can drop anything to win gold. But pls ensur...</td>\n",
       "      <td>1424333312454135812</td>\n",
       "      <td>2021-08-08T11:34:59.000Z</td>\n",
       "      <td>1424333312454135812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Is it a bird?! Is it a plane?!\\nNo it's some o...</td>\n",
       "      <td>1425760249936588802</td>\n",
       "      <td>2021-08-12T10:05:07.000Z</td>\n",
       "      <td>1425760249936588802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is a lot of information about tweets that you recieve can recieve from the API. We have narrowed it down to the important ones for this study:\n",
    "\n",
    "- text - The text of the tweet sent out. This will include user handles, hastags, emojis, and any retweet indicators\n",
    "- id - The unique id given to the tweet. This allows for twitter to store tweets.\n",
    "- created_at - When the tweet was tweeted. Given in UTC time.\n",
    "- conversation_id - The unique id given to twitter conversations. We can use this id to get all tweets in an conversation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\section{Data Cleaning}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Initial Exploration and Cleaning}\n",
    "\n",
    "So, now we have tweets regarding the olympic sports of basketball, biking, diving, gymnastics, skateboarding, surfing, track, and volleyball. All of the tweets were collected into a single dataframe.\n",
    "\n",
    "Now we have all of the tweets in single dataframe, making it easier for us to look at all of the tweets and clean them in bunches. During this process a new variable was created, sport. This varibale will track for us the sport that each tweet is talking about. When we make our sentiment analysis model later this will be very handy when anayzing by sport.\n",
    "\n",
    "Using the shape function we find that we have 30554 tweets. Now, time to get our hands dirty and clean the data.\n",
    "\n",
    "As with all text data there is alot of cleaning that can be done. For this project we will be concentrating on removal of certain entities such as url links and hashtags, removal of stopwords, lemmatization, and general NLP text cleaning. After this NLP specific cleaning we should be ready to analyze the sentiment of tweets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Type Conversions}\n",
    "\n",
    "We needed to change a few data types. Thankfully, the tweet text was already given to us as an object (string). To make our analysis easier we changed the tweet id and the conversation ids of type int from type object. To facilitate the time aspect of our analysis we needed to also covert the created_at variable to a datetime object.\n",
    "\n",
    "This leaves us with these variable types, the correct variable types.\n",
    "\n",
    "\\begin{table}\n",
    "\\begin{tabular}{c|c}\n",
    "id & int32\\\\\n",
    "created_at & datetime64[ns, UTC]\\\\\n",
    "conversation_id & int32\\\\\n",
    "text & object\\\\\n",
    "sport & object\\\\\n",
    "withheld & object\\\\\n",
    "clean_text & object\\\\\n",
    "clean_no_stops & object\\\\\n",
    "lemma_text & object\\\\\n",
    "\\end{tabular}\n",
    "\\end{table}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Outliers}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Additional Exploration}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\section{Model Creation}\n",
    "\\subsection{Model}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\section{Evaluation and Conclusions}\n",
    "\\subsection{Results}\n",
    "\\subsection{Future Work}"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "d5740b0c65ce1535d49f8104bf53e2dbf28aba8cf44c23304f5a7991b903c1ba"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}