{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{center}\n",
    "    BY\\\\\n",
    "    Troy Jennings\\\\\n",
    "    Carl Ausmees\\\\\n",
    "    Corey Vorsanger\n",
    "\\end{center}\n",
    "\n",
    "\\begin{center}\n",
    "    \\textit{For a more interactive report experience please go to \\href{https://github.com/cvorsanger/COMP-4447-Final-Project}{https://github.com/cvorsanger/COMP-4447-Final-Project} and launch the binder link}\n",
    "\\end{center}\n",
    "\n",
    "\\newpage\n",
    "\n",
    "\\tableofcontents"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\section{Introduction}\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Research Question}\n",
    "\n",
    "Every four years (five years sometimes) something special happens. No, it is not the world's greatest athletes coming together to showcase elite human athletisism in\n",
    "the Summer Olympics. Rather, it is the millions of people that come together to tweet about these athletes. In a showcase of exceptional human thumbs, these people tell you all you need to know about the Olympics; with 100% accuracy of course. \n",
    "\n",
    "So what does the twitter-sphere have to say about the Olympics? Are there sports that are being talked about in a better light then others? How does the sentiment of the Olympics change over the course of the event? In this project we will be investigating tweet sentiment involving Olympic sports to see how they evolve over time. In the end the sentiment of specfic sports will be analyzed and any trends should be discovered."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Literature Review}\n",
    "\n",
    "Text sentiment analysis is really nothing new. With the advancements of Natuaral Languauge Processing (NLP) techniques building a sentiment analysis tool as become common place in a wide variety of applications. While sentiment analysis is not limited to Twitter and can be used for any text data, with its vast amount of text data and constant addition of data Twitter has been used in many sentiment analysis applications. From using Twitter data to help predict stock data as in [Anshul Mittal](http://cs229.stanford.edu/proj2011/GoelMittal-StockMarketPredictionUsingTwitterSentimentAnalysis.pdf) to the always civil discussions about politics as shown in the [Geeks for Geeks article](https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/) and [TSAM](https://ieeexplore.ieee.org/abstract/document/6581022?casa_token=qrfkpDZP30sAAAAA:PNpsFf2_T9jXUB81SKLMldZji2tDprsCz4Ec4QSrHxlJQNIW3Yi52tWHZ4jhfTgPrqRjzjdKBfA) . Unsurprisingly, there has been work done in the Olympic domain. The closest work we have discovered documented in a paper was in [2016 Olympic Games on Twitter: Sentiment Analysis of Sports Fans Tweets using Big Data Framework](../Literature%20Review/JACET_Volume%205_Issue%203_Page%20143-160.pdf). This paper came out of the Ilamic Azad University in Iran. The writter focus on Iranian Olympians instead of sports. Using tweets in both English and Farsi they classify tweets as fearful, angry, surprising, sadness, joyful, neatruel, and anticipation. We will be only concerend about classifying tweets as negative, positive, or neutral.\n",
    "\n",
    "While most work follows the same general outline there are differences in sentiement analysis works. For instance [2016 Olympic Games on Twitter: Sentiment Analysis of Sports Fans Tweets using Big Data Framework](../Literature%20Review/JACET_Volume%205_Issue%203_Page%20143-160.pdf) uses the WordNet package to handle most of their preprocessing and sentiment analysis model. [Geeks for Geeks article](https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/) uses the TextBlob library for their sentiment analysis. [Yalin Yener](https://towardsdatascience.com/step-by-step-twitter-sentiment-analysis-in-python-d6f650ade58d) introduces the Vader sentiment engine in the nltk library.\n",
    "\n",
    "After review, we have decide to prusue using mostly the Natural Language ToolKit(NLTK) and the Textblob libraries. With these libraries we should be able to do most of the data cleaning and allows us to use the Vader sentiment engine. More information on Vader will be presented in the Model Creation section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\section{Data Ingestion}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Motivation}\n",
    "\n",
    "A particular interest of the authors are the ongoing Summer Olympic games. We enjoy watching the top athletes in the world compete in sports that are not on television very often; sports like, gymnastics,swimming, and track. A natural curioisty than was to see how other people view the Olympics.As social media has grown Twitter has been the go to platform for the world to express their views. Twitter allows for people to express their feelings on just about any subject they desire (for better or for worst). It would make sence then to look at Twitter data to model the sentiment around the Olympics. Luckily, Twitter provides an API  for us to query historical tweet data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Ingestion}\n",
    "\n",
    "he twitter API is a relative easy to use API. There are a few restictions such as rate limits and API types that you do have to consider. For the purposes of this study, the API makes it easy to look at historical tweets containing specific words and hashtags. The follow details certain aspects of the Twitter API that pertains to this project. For more information please visit \\href{https://developer.twitter.com/en/docs/twitter-api}{here}\n",
    "\n",
    "The first step is to get authorization. To get this we must provide the API with our given client key and secret using the POST command with the requests library. Once we get POST we will recieve a response from the API we must save."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests \n",
    "import base64\n",
    "# Save Authorization Info.\n",
    "client_key = 'XXXXXXXXXXXXXXXXXXXXXXXXXX' \n",
    "client_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXX' \n",
    "bearer_token = 'XXXXXXXXXXXXXXXXXXXXXXXXXX' \n",
    "key_secret = '{}:{}'.format(client_key, client_secret).encode('ascii')\n",
    "b64_encoded_key = base64.b64encode(key_secret)\n",
    "b64_encoded_key = b64_encoded_key.decode('ascii')\n",
    "# Build API URL \n",
    "base_url = 'https://api.twitter.com/'\n",
    "auth_endpoint = base_url + 'oauth2/token'\n",
    "auth_headers = { 'Authorization': 'Basic {}'.format(b64_encoded_key),\n",
    "                'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8' }\n",
    "auth_data = { 'grant_type': 'client_credentials' }\n",
    "# Provide Authorization Info and Save Access Token\n",
    "response = requests.post(auth_endpoint, headers=auth_headers, data=auth_data)\n",
    "print(\"Response Status Code: \",response.status_code)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected we get a response status code of \"200\". This tells us that we succesfully recieved our access token.\n",
    "\n",
    "The next step is to get the *access_token* from our authorization response."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A utility function was then created. This function allows us to use the twitter API to save tweets. The function needs the users saved access token and allows for the user to specify the number of tweest to pull. The most important parameter of this function is the query parameter. This allows for you filter the tweets you recieve by specific hashtags, words, retweets, etc. For this project we are concerned with tweets containg the hashtags \"#olympics\" and containing the a specific sport."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_tweets(access_token, query, max_tweets=10, tweet_limit=10):\n",
    "    \"\"\"Retrieve tweets from the recent search API.\n",
    "        Args\n",
    "        ----\n",
    "        access_token (str): A valid bearer token for making Twitter API requests.\n",
    "        query (str): A valid Twitter query string for filtering the search tweets.\n",
    "        max_tweets (int): The maximum number of tweets to collect in total.\n",
    "        tweet_limit (int): The number of maximum tweets per API request. \n",
    "    \"\"\"\n",
    "    page_token = None\n",
    "    tweet_data = []\n",
    "    search_headers = {\n",
    "        'Authorization': 'Bearer {}'.format(access_token),\n",
    "        'User-Agent': 'v2FullArchiveSearchPython',\n",
    "    }\n",
    "    # Divides the max tweets into the appropriate number of requests based on the tweet_limit.\n",
    "    for i in range(max_tweets // tweet_limit - 1):\n",
    "        search_parameters = {\n",
    "            'query': query,\n",
    "            'max_results': tweet_limit,\n",
    "            'tweet.fields': 'lang,created_at,referenced_tweets,source,conversation_id'\n",
    "        }\n",
    "                # If we reach the 2nd page of results, add a next_token attribute to the search parameters\n",
    "        if i > 0:\n",
    "            search_parameters['next_token'] = page_token\n",
    "\n",
    "        response = requests.get(search_url, headers=search_headers, params=search_parameters)\n",
    "        if response.status_code != 200:\n",
    "            print(f'\\tError occurred: Status Code{response.status_code}: {response.text}')\n",
    "        else:\n",
    "            # We need to check for a result count before doing anything futher; if we have result_count we have data\n",
    "            if response.json()['meta']['result_count'] > 0:\n",
    "                tweet_data.extend(response.json()['data'])\n",
    "                \n",
    "                # If a 'next_token' exists, then update the page token to continue pagination through results\n",
    "                if 'next_token' in response.json()['meta']:\n",
    "                    page_token = response.json()['meta']['next_token']\n",
    "            else:\n",
    "                print(f'\\tNo data returned for query!')\n",
    "                break        \n",
    "        print(f'\\t{len(tweet_data)} total tweets gathered')\n",
    "        time.sleep(1)\n",
    "    return tweet_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the above function we can sample retweets having to do with olympic gymnastics. Notice that we are specify the hashtag \"#olympics\", the word \"gymnastics\" and is a retweet in the query parameter. Finally, so we do have to keep requesting data from the twitter API we save the tweets in a pickle file. This way we can use them later in our anaysis (and twitter will not get mad at us for rate limits)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make the request\n",
    "q = '#olympics gymnastics -is:retweet'\n",
    "olympic_tweets = get_tweets(access_token, q, max_tweets=5000, tweet_limit=100)\n",
    "\n",
    "with open('../data/gymnastics-tweets.pkl', 'wb') as f:\n",
    "\tpickle.dump(olympic_tweets, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the \"get_tweets\" function we tried to pull 5000 tweets for the sports basketball, biking, diving, gymnastics, skateboarding, surfing, track, and volleyball. This was easily done by changing the query parameter to include the name of the sport."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\subsection{Sample and Explanation}\n",
    "\n",
    "Alright we have pulled in some tweets. Now lets have a look at some of the original tweets and the retweets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "original_tweets_df = pd.DataFrame(pd.read_pickle('../data/gymnastics-tweets.pkl'))\n",
    "original_tweets_df.sample(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text                   id  \\\n",
       "1616  @ttperryplay716 Thank You, Simone Biles - Head...  1423370946782584835   \n",
       "1336  My approach to the #Olympics is to watch sport...  1423482327620014080   \n",
       "3016  @Anthony_C_Wray Thank You, Simone Biles - Head...  1423371072855056387   \n",
       "3949  #Olympics gymnastics champion Suni Lee makes a...  1423375229645205505   \n",
       "2562  @RaelleLogan1 Thank You, Simone Biles - Headph...  1423371302803542018   \n",
       "\n",
       "                    created_at      conversation_id  \n",
       "1616  2021-08-05T19:50:53.000Z  1423262842669740039  \n",
       "1336  2021-08-06T03:13:28.000Z  1423482327620014080  \n",
       "3016  2021-08-05T19:51:23.000Z  1423299426140016642  \n",
       "3949  2021-08-05T20:07:54.000Z  1423375229645205505  \n",
       "2562  2021-08-05T19:52:18.000Z  1423262559201898498  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>conversation_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>@ttperryplay716 Thank You, Simone Biles - Head...</td>\n",
       "      <td>1423370946782584835</td>\n",
       "      <td>2021-08-05T19:50:53.000Z</td>\n",
       "      <td>1423262842669740039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>My approach to the #Olympics is to watch sport...</td>\n",
       "      <td>1423482327620014080</td>\n",
       "      <td>2021-08-06T03:13:28.000Z</td>\n",
       "      <td>1423482327620014080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>@Anthony_C_Wray Thank You, Simone Biles - Head...</td>\n",
       "      <td>1423371072855056387</td>\n",
       "      <td>2021-08-05T19:51:23.000Z</td>\n",
       "      <td>1423299426140016642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>#Olympics gymnastics champion Suni Lee makes a...</td>\n",
       "      <td>1423375229645205505</td>\n",
       "      <td>2021-08-05T20:07:54.000Z</td>\n",
       "      <td>1423375229645205505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>@RaelleLogan1 Thank You, Simone Biles - Headph...</td>\n",
       "      <td>1423371302803542018</td>\n",
       "      <td>2021-08-05T19:52:18.000Z</td>\n",
       "      <td>1423262559201898498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is  a lot of information about tweets that you recieve. For us the most important ones are:\n",
    "\n",
    "- text - The text of the tweet sent out. This will include user handles, hastags, emojis, and any retweet indicators\n",
    "- id - The unique id given to the tweet. This allows for twitter to store tweets.\n",
    "- created_at - When the tweet was tweeted. Given in UTC time.\n",
    "- conversation_id - The unique id given to twitter conversations. We can use this id to get all tweets in an conversation.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\section{Data Cleaning}\n",
    "\\subsection{Initial Exploration and Cleaning}\n",
    "\\subsection{Type Conversions}\n",
    "\\subsection{Outliers}\n",
    "\\subsection{Additional Exploration}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\section{Model Creation}\n",
    "\\subsection{Model}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\section{Evaluation and Conclusions}\n",
    "\\subsection{Results}\n",
    "\\subsection{Future Work}"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "d5740b0c65ce1535d49f8104bf53e2dbf28aba8cf44c23304f5a7991b903c1ba"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}